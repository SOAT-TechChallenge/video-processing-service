import os
import requests
import asyncio
import re
import logging
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import List, Dict, Any
import aiofiles
import time

from .s3_service import S3Service
from .sqs_consumer import SQSConsumer
from .utils import (
    extract_frames_from_video,
    create_zip_from_images,
    generate_unique_id,
    cleanup_temp_files
)
from .schemas import ProcessingStatus
from .config import S3_BUCKET_NAME, UPLOAD_DIR, OUTPUT_DIR, SQS_QUEUE_URL

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configura√ß√µes do Notification Service (Lidas do Environment do Kubernetes)
NOTIFICATION_SERVICE_URL = os.getenv('NOTIFICATION_SERVICE_URL')
API_TOKEN = os.getenv('API_SECURITY_INTERNAL_TOKEN')

if not NOTIFICATION_SERVICE_URL:
    logger.warning("‚ö†Ô∏è NOTIFICATION_SERVICE_URL n√£o definida! Notifica√ß√µes n√£o funcionar√£o.")

if not API_TOKEN:
    logger.error("‚ùå API_SECURITY_INTERNAL_TOKEN n√£o definido! Falha de seguran√ßa cr√≠tica.")

class VideoProcessor(SQSConsumer):
    def __init__(self, upload_dir: str = UPLOAD_DIR, output_dir: str = OUTPUT_DIR):
        
        # Configurar SQS (se houver URL configurada)
        sqs_queue_url = SQS_QUEUE_URL
        if sqs_queue_url:
            super().__init__(sqs_queue_url)
        else:
            # Inicializar sem SQS se n√£o configurado
            self.queue_url = None
            logger.warning("‚ö†Ô∏è SQS_QUEUE_URL n√£o configurada - Consumidor SQS desativado")
        
        # Configurar S3
        self.s3_bucket = S3_BUCKET_NAME
        self.s3_service = S3Service()
        
        # Configurar diret√≥rios
        self.upload_dir = Path(upload_dir)
        self.output_dir = Path(output_dir)
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.executor = ThreadPoolExecutor(max_workers=5)
        self.is_consuming = False
        
        logger.info(f"üé¨ VideoProcessor inicializado")
        logger.info(f"üì¶ S3 Bucket: {self.s3_bucket}")
        if sqs_queue_url:
            logger.info(f"üì´ SQS Queue: {sqs_queue_url}")
        logger.info(f"üìÅ Upload dir: {upload_dir}, Output dir: {output_dir}")
        logger.info(f"üîî Notification URL: {NOTIFICATION_SERVICE_URL}")

    def _send_error_email(self, email: str, title: str, error_message: str):
        """Envia notifica√ß√£o de erro via HTTP para o Notification Service"""
        if not email or not NOTIFICATION_SERVICE_URL:
            logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel enviar notifica√ß√£o (Email ou URL ausente)")
            return

        try:
            # Rota ajustada conforme solicitado
            url = f"{NOTIFICATION_SERVICE_URL}/api/notification/send-email"
            
            # Payload ajustado (usando 'body' conforme solicitado)
            payload = {
                "to": email,
                "subject": f"Falha no processamento do v√≠deo: {title}",
                "body": f"Ol√°, infelizmente ocorreu um erro ao processar seu v√≠deo.\nErro: {error_message}"
            }

            headers = {
                "Content-Type": "application/json",
                "x-apigateway-token": API_TOKEN # Token para passar pelo ALB
            }

            logger.info(f"üìß Enviando notifica√ß√£o de erro para: {email}")
            # Timeout curto (5s) para n√£o travar o processamento se o notification cair
            response = requests.post(url, json=payload, headers=headers, timeout=5)
            
            if response.status_code == 200:
                logger.info("‚úÖ Notifica√ß√£o enviada com sucesso!")
            else:
                logger.error(f"‚ùå Falha ao enviar notifica√ß√£o: {response.status_code} - {response.text}")

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao chamar Notification Service: {e}")

    async def process_message(self, message: Dict[str, Any]) -> bool:
        """Processa uma mensagem da fila SQS"""
        try:
            s3_key = message.get('s3Key')
            s3_url = message.get('s3Url')
            title = message.get('title', 'Untitled')
            description = message.get('description', '')
            uploaded_at = message.get('uploadedAt')
            email = message.get('email') # <--- Pega o email da mensagem
            
            if not s3_key:
                logger.error("‚ùå Mensagem sem s3Key")
                return False
            
            logger.info(f"üì© Nova mensagem SQS recebida:")
            logger.info(f"   üìÇ Arquivo: {s3_key}")
            logger.info(f"   üìù T√≠tulo: {title}")
            logger.info(f"   üë§ Email: {email}")
            logger.info(f"   üïê Uploaded at: {uploaded_at}")
            
            # Processar v√≠deo do S3
            result = await self.process_video_from_s3(
                s3_key=s3_key,
                title=title,
                description=description,
                source="sqs"
            )
            
            if result.get("status") == ProcessingStatus.COMPLETED:
                logger.info(f"‚úÖ Processamento via SQS conclu√≠do: {result.get('video_id')}")
                return True
            else:
                # L√≥gica de Falha: Loga o erro e tenta notificar o usu√°rio
                error_msg = result.get('error', 'Erro desconhecido')
                logger.error(f"‚ùå Falha no processamento via SQS: {error_msg}")
                
                if email:
                    logger.info("üöÄ Iniciando envio de notifica√ß√£o de erro...")
                    # Executa o requests (s√≠ncrono) em uma thread separada para n√£o bloquear o loop async
                    await asyncio.get_event_loop().run_in_executor(
                        self.executor, 
                        self._send_error_email, 
                        email, 
                        title, 
                        error_msg
                    )
                else:
                    logger.warning("‚ö†Ô∏è Email n√£o encontrado na mensagem, notifica√ß√£o pulada.")

                return False
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar mensagem SQS: {e}")
            return False
    
    async def process_video_from_s3(self, s3_key: str, title: str = "Unknown", 
                                    description: str = "", user_id: str = "system",
                                    source: str = "manual") -> dict:
        """Processa um v√≠deo espec√≠fico do S3 (para testes manuais ou SQS)"""
        try:
            logger.info(f"üöÄ Iniciando processamento do v√≠deo do S3")
            logger.info(f"   üìÇ Arquivo: {s3_key}")
            logger.info(f"   üìù T√≠tulo: {title}")
            logger.info(f"   üì¶ Bucket: {self.s3_bucket}")
            logger.info(f"   üì° Fonte: {source}")
            
            # Verificar se o v√≠deo existe no S3
            if not self.s3_service.video_exists(s3_key):
                logger.error(f"‚ùå V√≠deo n√£o encontrado no S3: {s3_key}")
                return {
                    "video_id": "not_found",
                    "status": ProcessingStatus.FAILED,
                    "error": f"V√≠deo n√£o encontrado no S3: {s3_key}",
                    "s3_key": s3_key
                }
            
            # Baixar v√≠deo do S3
            video_filename = f"{generate_unique_id()}_{Path(s3_key).name}"
            video_path = self.upload_dir / video_filename
            
            logger.info(f"‚¨áÔ∏è Baixando v√≠deo do S3...")
            self.s3_service.download_video(
                s3_key=s3_key,
                local_path=str(video_path)
            )
            
            logger.info(f"‚úÖ V√≠deo baixado: {video_path.name}")
            
            # Processar v√≠deo
            result = await self._process_video_internal(
                video_path=str(video_path),
                user_id=user_id,
                video_metadata={
                    's3_key': s3_key,
                    'title': title,
                    'description': description,
                    'bucket': self.s3_bucket,
                    'source': source,
                    'original_filename': Path(s3_key).name
                }
            )
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar v√≠deo do S3: {e}")
            return {
                "video_id": "error",
                "status": ProcessingStatus.FAILED,
                "error": str(e),
                "s3_key": s3_key
            }
    
    async def _process_video_internal(self, video_path: str, user_id: str, 
                                     video_metadata: Dict = None) -> dict:
        """Processa um v√≠deo baixado do S3"""
        video_id = None
        try:
            logger.info(f"üîÑ Processando v√≠deo local: {Path(video_path).name}")
            
            # Extrair video_id do nome do arquivo
            video_id = Path(video_path).stem.split('_')[0]
            
            # Criar diret√≥rio tempor√°rio
            temp_dir = self.output_dir / f"temp_{video_id}"
            temp_dir.mkdir(exist_ok=True)
            
            logger.info(f"üéûÔ∏è Extraindo frames do v√≠deo...")
            # Extrair frames do v√≠deo
            frame_paths = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                extract_frames_from_video,
                video_path,
                str(temp_dir),
                1  # 1 frame por segundo
            )
            
            logger.info(f"üìä Frames extra√≠dos: {len(frame_paths)}")
            
            if not frame_paths:
                raise ValueError("N√£o foi poss√≠vel extrair frames do v√≠deo")
            
            # Adicionar metadados ao nome do ZIP
            title_safe = re.sub(r'[^\w\.-]', '_', video_metadata.get('title', 'video'))
            zip_filename = f"{video_id}_{title_safe}_frames.zip"
            zip_path = self.output_dir / zip_filename
            
            logger.info(f"üì¶ Criando arquivo ZIP: {zip_path.name}")
            await asyncio.get_event_loop().run_in_executor(
                self.executor,
                create_zip_from_images,
                frame_paths,
                str(zip_path)
            )
            
            # Limpar arquivos tempor√°rios
            cleanup_temp_files(video_path, str(temp_dir))
            
            logger.info(f"‚úÖ Processamento conclu√≠do!")
            
            return {
                "video_id": video_id,
                "status": ProcessingStatus.COMPLETED,
                "zip_filename": zip_filename,
                "zip_path": str(zip_path),
                "zip_url": f"/download/{zip_filename}",
                "frame_count": len(frame_paths),
                "error": None,
                "metadata": video_metadata,
                "processing_time": time.time()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar v√≠deo: {str(e)}")
            
            # Limpar em caso de erro
            if video_path and os.path.exists(video_path):
                cleanup_temp_files(video_path)
            
            return {
                "video_id": video_id or "unknown",
                "status": ProcessingStatus.FAILED,
                "zip_filename": None,
                "zip_path": None,
                "frame_count": None,
                "error": str(e),
                "metadata": video_metadata
            }
    
    def list_available_videos(self, prefix: str = "videos/") -> List[Dict]:
        """Lista v√≠deos dispon√≠veis no S3"""
        try:
            return self.s3_service.list_videos(prefix)
        except Exception as e:
            logger.error(f"‚ùå Erro ao listar v√≠deos do S3: {e}")
            return []
    
    def get_processed_files(self) -> List[Dict]:
        """Lista todos os arquivos ZIP processados"""
        try:
            zip_files = list(self.output_dir.glob("*.zip"))
            
            files = []
            for zip_file in zip_files:
                files.append({
                    "filename": zip_file.name,
                    "size": zip_file.stat().st_size,
                    "created_at": zip_file.stat().st_ctime,
                    "path": str(zip_file),
                    "url": f"/download/{zip_file.name}"
                })
            
            return files
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao listar arquivos processados: {e}")
            return []
    
    async def start_sqs_consumer(self):
        """Inicia o consumo cont√≠nuo da fila SQS (se configurado)"""
        if not self.queue_url:
            logger.warning("‚ö†Ô∏è Consumidor SQS n√£o iniciado (queue_url n√£o configurada)")
            return
        
        self.is_consuming = True
        logger.info(f"üì´ Iniciando consumidor SQS...")
        
        while self.is_consuming:
            try:
                await self.consume_messages()
                await asyncio.sleep(5)
            except Exception as e:
                logger.error(f"‚ùå Erro no consumidor SQS: {e}")
                await asyncio.sleep(30)
    
    def stop_sqs_consumer(self):
        """Para o consumo da fila SQS"""
        self.is_consuming = False
        logger.info("üõë Consumidor SQS parado")